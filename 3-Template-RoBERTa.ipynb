{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLBSE'24 RoBERTa Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"roberta-base\"\n",
    "RANDOM_SEED = 42\n",
    "OUTPUT_PATH = 'output/roberta'\n",
    "\n",
    "!mkdir -p $OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repo', 'created_at', 'label', 'title', 'body'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['repo', 'created_at', 'label', 'title', 'body'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds = Dataset.from_csv({ \"train\": \"data/issues_train.csv\", \"test\": \"data/issues_test.csv\" })\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['facebook/react', 'tensorflow/tensorflow', 'microsoft/vscode', 'bitcoin/bitcoin', 'opencv/opencv']\n"
     ]
    }
   ],
   "source": [
    "repos = ds[\"train\"].unique(\"repo\")\n",
    "print(repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>bug</th>\n",
       "      <th>feature</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>repo</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bitcoin/bitcoin</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facebook/react</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft/vscode</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opencv/opencv</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tensorflow/tensorflow</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label                  bug  feature  question\n",
       "repo                                         \n",
       "bitcoin/bitcoin        100      100       100\n",
       "facebook/react         100      100       100\n",
       "microsoft/vscode       100      100       100\n",
       "opencv/opencv          100      100       100\n",
       "tensorflow/tensorflow  100      100       100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].to_pandas().groupby([\"repo\", \"label\"]).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "label2id = { \"bug\": 0, \"feature\": 1, \"question\": 2 }\n",
    "id2label = { 0: \"bug\", 1: \"feature\", 2: \"question\" }\n",
    "\n",
    "def process_dataset(example):\n",
    "\n",
    "    # concatenate title and body\n",
    "    text = example['title'] or \"\" + \" \" + example['body'] or \"\"\n",
    "\n",
    "    # Remove strings between triple quotes\n",
    "    text = re.sub(r'```.*?```', ' ', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove new lines\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "\n",
    "    # Remove links\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)\n",
    "\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "\n",
    "    # Remove special characters except the question marks\n",
    "    text = re.sub(r'[^a-zA-Z0-9?\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    example['text'] = text\n",
    "\n",
    "    return example\n",
    "\n",
    "ds = ds.shuffle(seed=RANDOM_SEED)\n",
    "ds = ds.map(process_dataset)\n",
    "ds = ds.select_columns(['repo', 'label', 'text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "def create_model(max_length=256, truncation=True, padding=\"max_length\", device=\"cuda\"):\n",
    "  # The tokenizer is based on Roberta. The configurations are: Max_length = 256, truncation = true, padding = max_length.\n",
    "  tokenizer = RobertaTokenizer.from_pretrained(BASE_MODEL, device=device, max_length=max_length, truncation=truncation, padding=padding)\n",
    "\n",
    "  # Configuration: We have 3 labels: Bug, Enhancment, Question.\n",
    "  config = RobertaConfig.from_pretrained(BASE_MODEL, device=device, num_labels=3)\n",
    "\n",
    "  # Configuration of the Adapter model.\n",
    "  model = RobertaForSequenceClassification.from_pretrained(BASE_MODEL, config=config)\n",
    "  \n",
    "  # This part is for inferencing\n",
    "  classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=device, max_length=max_length, padding=padding, truncation=truncation)\n",
    "\n",
    "  return tokenizer, model, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 5793.56 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:08, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:00<00:00, 5593.52 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 5458.30 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:10, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:00<00:00, 6181.15 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 5579.58 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:06, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:00<00:00, 6818.60 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 5296.66 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:07, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:00<00:00, 6588.50 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 5740.96 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:10, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 300/300 [00:00<00:00, 6792.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "def repo_eq(repo: str):\n",
    "    return lambda example: example['repo'] == repo\n",
    "\n",
    "references = {}\n",
    "predictions = {}\n",
    "\n",
    "for repo in repos:\n",
    "\n",
    "    tokenizer, model, classifier = create_model(max_length=512)\n",
    "\n",
    "    train_set = ds.filter(repo_eq(repo))[\"train\"]\n",
    "    train_set = train_set.class_encode_column(\"label\")\n",
    "    train_set = train_set.map(lambda batch: tokenizer(batch[\"text\"]), batched=True)\n",
    "    train_set.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_PATH,\n",
    "        num_train_epochs=20,\n",
    "        per_device_train_batch_size=256,\n",
    "        save_strategy=\"no\",\n",
    "        evaluation_strategy=\"no\",\n",
    "        seed=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=train_set,\n",
    "        eval_dataset=None,\n",
    "        \n",
    "        data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    test_set = ds.filter(repo_eq(repo))[\"test\"]\n",
    "    references[repo] = test_set['label']\n",
    "    test_set = test_set.map(lambda batch: tokenizer(batch[\"text\"]), batched=True)\n",
    "    test_set.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    predictions[repo] = classifier(test_set['text'])\n",
    "    predictions[repo] = [pred['label'] for pred in predictions[repo]]\n",
    "    predictions[repo] = [model.config.label2id[label] for label in predictions[repo]]\n",
    "    predictions[repo] = [id2label[id] for id in predictions[repo]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from numpy import mean\n",
    "\n",
    "results = {}\n",
    "metrics = ['precision', 'recall', 'f1-score']\n",
    "labels = ['bug', 'feature', 'question']\n",
    "\n",
    "for repo in repos:\n",
    "  results[repo] = classification_report(references[repo], predictions[repo], digits=4, output_dict=True)\n",
    "  results[repo]['average'] = results[repo]['weighted avg']\n",
    "  results[repo] = {label: {metric: results[repo][label][metric] for metric in metrics} for label in labels + ['average']}\n",
    "\n",
    "results['overall'] = {label: {metric: mean([results[repo][label][metric] for repo in repos]) for metric in metrics} for label in labels + ['average']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository               Label     Precision  Recall     F1\n",
      "---------------------------------------------------------------\n",
      "facebook/react           bug       0.8585     0.9100     0.8835     \n",
      "facebook/react           feature   0.7170     0.7600     0.7379     \n",
      "facebook/react           question  0.7386     0.6500     0.6915     \n",
      "facebook/react           average   0.7714     0.7733     0.7709     \n",
      "---------------------------------------------------------------\n",
      "tensorflow/tensorflow    bug       0.6875     0.7700     0.7264     \n",
      "tensorflow/tensorflow    feature   0.6514     0.7100     0.6794     \n",
      "tensorflow/tensorflow    question  0.5443     0.4300     0.4804     \n",
      "tensorflow/tensorflow    average   0.6277     0.6367     0.6288     \n",
      "---------------------------------------------------------------\n",
      "microsoft/vscode         bug       0.7250     0.5800     0.6444     \n",
      "microsoft/vscode         feature   0.6518     0.7300     0.6887     \n",
      "microsoft/vscode         question  0.6667     0.7200     0.6923     \n",
      "microsoft/vscode         average   0.6812     0.6767     0.6751     \n",
      "---------------------------------------------------------------\n",
      "bitcoin/bitcoin          bug       0.6111     0.6600     0.6346     \n",
      "bitcoin/bitcoin          feature   0.7857     0.6600     0.7174     \n",
      "bitcoin/bitcoin          question  0.5093     0.5500     0.5288     \n",
      "bitcoin/bitcoin          average   0.6354     0.6233     0.6270     \n",
      "---------------------------------------------------------------\n",
      "opencv/opencv            bug       0.6167     0.7400     0.6727     \n",
      "opencv/opencv            feature   0.6989     0.6500     0.6736     \n",
      "opencv/opencv            question  0.6552     0.5700     0.6096     \n",
      "opencv/opencv            average   0.6569     0.6533     0.6520     \n",
      "---------------------------------------------------------------\n",
      "overall                  bug       0.6998     0.7320     0.7123     \n",
      "overall                  feature   0.7010     0.7020     0.6994     \n",
      "overall                  question  0.6228     0.5840     0.6005     \n",
      "overall                  average   0.6745     0.6727     0.6708     \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "output_file_name = 'results.json'\n",
    "\n",
    "with open(os.path.join(OUTPUT_PATH, output_file_name), 'w') as fp:\n",
    "    json.dump(results, fp, indent=2)\n",
    "\n",
    "print(f\"Repository{' '*15}Label     Precision  Recall     F1\")\n",
    "for repo in repos + ['overall']:\n",
    "  print(\"-\"*63)\n",
    "  for label in labels + ['average']:\n",
    "    out = f\"{repo:<25}{label:<10}\"\n",
    "    for metric in metrics:\n",
    "      out += f\"{results[repo][label][metric]:<10.4f} \"\n",
    "    print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
